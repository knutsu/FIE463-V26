{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 1: Data cleaning\n",
        "\n",
        "Before doing actual data analysis, we usually first need to clean the data. \n",
        "This might involve steps such as dealing with missing values and encoding categorical variables as integers.\n",
        "\n",
        "Load the Titanic data set in `titanic.csv` and perform the following tasks:\n",
        "\n",
        "1. Report the number of observations with missing `Age`, for example using \n",
        "   [`isna()`](https://pandas.pydata.org/docs/reference/api/pandas.isna.html).\n",
        "\n",
        "2. Compute the average age in the data set. Use the following approaches and compare your results:\n",
        "    1.  Use the [`mean()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.mean.html) method.\n",
        "    2.  Convert the `Age` column to a NumPy array using \n",
        "        [`to_numpy()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_numpy.html). \n",
        "        Experiment with NumPy's \n",
        "        [`np.mean()`](https://numpy.org/doc/2.0/reference/generated/numpy.mean.html) and \n",
        "        [`np.nanmean()`](https://numpy.org/doc/2.0/reference/generated/numpy.nanmean.html) \n",
        "        to see if you obtain the same results.\n",
        "\n",
        "3. Replace all missing ages with the mean age you computed above, rounded to the nearest integer. Convert this updated `Age` column to integer type using \n",
        "   [`astype()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.astype.html).\n",
        "\n",
        "   Note that in \"real\" applications, replacing missing values with sample means is usually not a good idea. \n",
        "\n",
        "4. Generate a new column `Female` which takes on the value one if `Sex` is equal to `\"female\"` and zero otherwise. \n",
        "   This is called an _indicator_ or _dummy_ variable, and is preferable to storing such categorical data as strings.\n",
        "   Delete the original column `Sex`.\n",
        "\n",
        "5. Save your cleaned data set as `titanic-clean.csv` using \n",
        "   [`to_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html) with `,` as the field separator.\n",
        "   Tell [`to_csv()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html) \n",
        "   to *not* write the `DataFrame` index to the CSV file as it's not needed in this example."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***\n",
        "# Exercise 2: Daily returns of US stock market indices\n",
        "\n",
        "In this exercise, we examine how the three major US stock market indices performed last year using data from Yahoo! Finance."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Use the [`yfinance`](https://pypi.org/project/yfinance/) library and its `download()` \n",
        "    function to obtain the time series of daily observations for the \n",
        "    [S&P 500](https://en.wikipedia.org/wiki/S%26P_500), the \n",
        "    [Dow Jones Industrial Average (DJIA)](https://en.wikipedia.org/wiki/Dow_Jones_Industrial_Average) \n",
        "    and the \n",
        "    [NASDAQ Composite](https://en.wikipedia.org/wiki/Nasdaq_Composite) \n",
        "    indices. \n",
        "    Restrict the sample to the period from 2025-01-01 to 2025-12-31\n",
        "    and keep only the closing price stored in column `Close`.\n",
        "\n",
        "    _Hint_: The corresponding ticker symbols are `^GSPC`, `^DJI`, `^IXIC`, respectively.\n",
        "\n",
        "2. Rename the DataFrame columns to `'SP500'`, `'Dow Jones'` and `'NASDAQ'` using the \n",
        "    [`rename()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.rename.html) \n",
        "    method.\n",
        "\n",
        "    *Hint:* `rename(columns=dict)` expects a dictionary as an argument which maps existing to new column names.\n",
        "\n",
        "3.  Plot the three time series (one for each index) in a single graph. Label all axes and make sure your graph contains a legend.\n",
        "\n",
        "    _Hint:_ You can directly use the [`DataFrame.plot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.plot.html) \n",
        "    method implemented in `pandas`.\n",
        "\n",
        "4.  The graph you created in the previous sub-question is not well-suited to illustrate how each index developed in 2025 \n",
        "    since the indices are reported on vastly different scales (the S&P500 appears to be an almost flat line).\n",
        "\n",
        "    To get a better idea about how each index fared in 2025 relative to its value at the beginning of the year, normalize each index by its value on the first trading day in 2025 (which was 2025-01-02). Plot the resulting normalized indices.\n",
        "\n",
        "5.  For each index, compute the daily returns, i.e., the relative change vs. the previous closing price in percent.\n",
        "    Create a plot of the daily returns for all indices.\n",
        "\n",
        "    *Hint:* Use [`pct_change()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pct_change.html) to compute the change \n",
        "    relative to the previous observation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***\n",
        "# Exercise 3: Daily returns of the magnificent seven\n",
        "\n",
        "In this exercise, you are asked to analyze the weekly stockmarket returns\n",
        "of the so-called magnificent 7 which are some of the most successful tech companies \n",
        "of the last decades:\n",
        "Apple (AAPL), Amazon (AMZN), Google (GOOG), Meta (META), Microsoft (MSFT), Nvidia (NVDA), and Tesla (TSLA).\n",
        "\n",
        "1. Load the CSV data from \n",
        "    `../../data/stockmarket/magnificent7.csv`. \n",
        "    Inspect to first few rows\n",
        "    to familiarize yourself with the columns present in the `DataFrame`.\n",
        "\n",
        "    Keep only the columns `Date`, `Ticker`, `Open`, and `Close`.\n",
        "\n",
        "2. You want to compute weekly returns for each of the 7 stocks. To this end, \n",
        "    you need to reshape the `DataFrame` so that `Date` is the index and the\n",
        "    remaining dimensions are in (hierarchical) columns.\n",
        "\n",
        "    One way to achieve this is to use the \n",
        "    [`pivot()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html)\n",
        "    functions. Call this function with the arguments `index='Date'` and \n",
        "    `columns='Ticker'` and inspect the result.\n",
        "\n",
        "    This should generate a hierarchical column index with `Open` and `Close`\n",
        "    and the top level.\n",
        "\n",
        "    Drop all rows with any missing values which arise because these\n",
        "    stocks have been listed at different points in time.\n",
        "\n",
        "3.  Your data is now in a format that can be resampled to weekly frequency.\n",
        "    Use \n",
        "    [`resample()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html)\n",
        "    to convert the data to weekly observations.\n",
        "\n",
        "    Compute the weekly returns as the relative difference between the \n",
        "    *first* `Open` quote and the *last* `Close` quote for each ticker in each week.\n",
        "\n",
        "    *Hint:* You should use `resample('W-MON')` so that the resampled weeks\n",
        "    begin on Mondays (as opposed to the default Sundays).\n",
        "\n",
        "    *Hint:* For example, to select the first `Open` value in each week, you should\n",
        "    use `resample('W-MON')['Open'].first()`.\n",
        "\n",
        "4.  Create a 3-by-3 figure and plot the weekly returns you computed for each ticker as a histogram,\n",
        "    using 25 bins (i.e., `bins=25` should be passed to the `hist()` function).\n",
        "\n",
        "    Since you have only 7 tickers but 9 subplots in the figure, the last \n",
        "    two remaining subplots should remain empty.\n",
        "\n",
        "    *Hint:* You can either use [`DataFrame.hist()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.hist.html) to plot the histogram, or Matplotlib's [`hist()`](https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.hist.html) function. In either case, you should add `density=True` such that the histogram is appropriately rescaled and comparable to the normal density.\n",
        "    \n",
        "5.  **[Advanced]**\n",
        "    Compare the histograms you created to the normal (Gaussian) probability \n",
        "    density function (PDF) to\n",
        "    get an idea how much weekly returns differ from a normal distribution.\n",
        "\n",
        "    First, compute mean and standard deviation for each ticker\n",
        "    and tabulate these.\n",
        "\n",
        "    Then add a line showing the normal PDF to each of the return histograms you created previously,\n",
        "    using the mean and standard deviation for each ticker.\n",
        "\n",
        "    *Hint:* Use the `pdf()` method of the [`scipy.stats.norm`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html#scipy.stats.norm) class to compute the normal density.\n",
        "\n",
        "6.  Finally, you are interested in how the weekly returns are correlated across \n",
        "    the 7 stocks. \n",
        "\n",
        "    Create a figure with 7-by-7 subplots showing the pairwise correlations \n",
        "    for each combination of stocks.\n",
        "\n",
        "    You can do this either with the\n",
        "    [`scatter_matrix()`](https://pandas.pydata.org/docs/reference/api/pandas.plotting.scatter_matrix.html) function contained in `pandas.plotting`, \n",
        "    or build the figure using Matplotlib functions.\n",
        "\n",
        "    **[Advanced]**\n",
        "    Additionally, use the \n",
        "    [DataFrame.corr()](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html)\n",
        "    method to compute the pairwise correlation matrix. Extract these values\n",
        "    and add them as text to each of the 7-by-7 subplots\n",
        "    (e.g., the correlation between returns on AAPL and AMZN is about 0.43,\n",
        "    so this text should be added to the subplot showing the \n",
        "    scatter plot of AAPL vs. AMZN).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***\n",
        "# Exercise 4: Business cycle correlations\n",
        "\n",
        "Use the macroeconomic data from the folder `../../data/FRED` to solve the following tasks:\n",
        "\n",
        "1.  There are seven decade-specific files named `FRED_monthly_19X0.csv` where `X` identifies the decade (`X` takes on the values 5, 6, 7, 8, 9, 0, 1). Write a loop that reads in all seven files as DataFrames and store them in a list.\n",
        "\n",
        "    *Hint:* Recall that you can use \n",
        "    `pd.read_csv(..., index_col='DATE', parse_dates=['DATE'])` \n",
        "    to automatically parse strings stored in the `DATE` column as dates.\n",
        "\n",
        "2.  Use \n",
        "    [`pd.concat()`](https://pandas.pydata.org/docs/reference/api/pandas.concat.html) to concatenate these data sets into a single `DataFrame` and make sure \n",
        "    that `DATE` is set as the index.\n",
        "\n",
        "3.  You realize that your data does not include GDP since this variable is only reported at quarterly frequency.\n",
        "    Load the GDP data from the file `GDP.csv` and merge it with your monthly data using an _inner join_.\n",
        "4.  You want to compute how (percent) changes of the variables in your data correlate with percent changes in GDP.\n",
        "\n",
        "    1. Create a _new_ `DataFrame` which contains the percent changes in CPI and GDP (using \n",
        "    [`pct_change()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pct_change.html)),\n",
        "    and the absolute changes for the remaining variables (using \n",
        "    [`diff()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.diff.html)).\n",
        "    \n",
        "    2.  Compute the correlation of the percent changes in GDP with the (percent) changes of all other variables (using [`corr()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html)). What does the sign and magnitude of the correlation coefficient tell you?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***\n",
        "# Exercise 5: Okun's law\n",
        "\n",
        "In this exercise, we investigate [Okun's law](https://en.wikipedia.org/wiki/Okun%27s_law) based on quarterly US data for each of the last seven decades.\n",
        "\n",
        "Okun's law relates unemployment to the output gap. One version (see Jones: Macroeconomics, 2019) is stated as follows:\n",
        "<a id=\"Okun\"></a> \n",
        "$$\n",
        "\\underbrace{u_t - \\overline{u}_{t}\\vphantom{\\left(\\frac{Y_t - \\overline{Y}_t}{\\overline{Y}_t}\\right)}}_{\\text{cyclical unempl.}} = \n",
        "\\alpha + \\beta \\;\n",
        "    \\underbrace{\\left(\\frac{Y_t - \\overline{Y}_t}{\\overline{Y}_t}\\right)}_{\\text{output gap}}\n",
        "\\tag{3.1}\n",
        "$$\n",
        "where $u_t$ is the unemployment rate, $\\overline{u}_{t}$ is the natural rate of \n",
        "unemployment, $Y_t$ is output (GDP) and $\\overline{Y}_{t}$ is potential output. \n",
        "We refer to $u_t-\\overline{u}_{t}$ as \"cyclical unemployment\" and to the term in \n",
        "parenthesis on the right-hand side as the \"output gap.\" Okun's law says that \n",
        "the coefficient $\\beta$ is negative, i.e., cyclical unemployment is \n",
        "higher when the output gap is low (negative) because the economy \n",
        "is in a recession.\n",
        "\n",
        "Use the FRED data in the `../../data/FRED` folder and \n",
        "perform the following tasks:\n",
        "\n",
        "1.  Load the time series stored in `GDP.csv` (real GDP), \n",
        "    `GDPPOT.csv` (real potential GDP),\n",
        "    `UNRATE.csv` (unemployment rate) and \n",
        "    `NROU.csv` (noncyclical rate of unemployment), where the last \n",
        "    series corresponds to the natural rate of unemployment mentioned above.\n",
        "\n",
        "    Combine these series into a single `DataFrame` so that each \n",
        "    represents a column, and keep only observations from from 1950-2019.\n",
        "    The resulting data should be at quarterly frequency since GDP is \n",
        "    only observed at these intervals.\n",
        "\n",
        "    *Hint:* Use `pd.read_csv(..., index_col='DATE', parse_dates=['DATE'])`\n",
        "    to automatically parse strings stored in the `DATE` column as dates\n",
        "    and set it as the index.\n",
        "\n",
        "2.  Compute the output gap and cyclical unemployment rate as defined above and \n",
        "    add them as columns to the `DataFrame`.\n",
        "\n",
        "    Plot these variables in a scatter plot\n",
        "    with the output gap on the $x$-axis and the \n",
        "    cyclical unemployment on the $y$-axis. Does Okun's law hold over the\n",
        "    sample period?\n",
        "\n",
        "3.  You wonder if the relationship has changed over the last decades. \n",
        "    To answer this question, create a new column `Decade` which stores the decade of each observation,\n",
        "    e.g., 1950, 1960, etc.\n",
        "    Verify that each decade has 40 quarterly observations in your data.\n",
        "\n",
        "    *Hint:* Since you have a date index, the calendar year can be \n",
        "    retrieved from the attribute `df.index.year`.\n",
        "\n",
        "4.  Create a figure with 3-by-3 subplots showing the same scatter plot as \n",
        "    above, but separately for each decade. Since we have data for only 7\n",
        "    decades, the last two subplots should remain empty.\n",
        "\n",
        "5.  **[Advanced]** \n",
        "    Write a function `regress_okun()` which accepts a `DataFrame` \n",
        "    containing a decade-specific \n",
        "    sub-sample as the only argument, and estimates the coefficients \n",
        "    $\\alpha$ (the intercept) and $\\beta$ (the slope) of the above regression\n",
        "    equation [(3.1)](#Okun).\n",
        "\n",
        "    This function should return a `Series` with two elements\n",
        "    which store the intercept and slope.\n",
        "\n",
        "    To run the regression by decade, group the data by `Decade` and call the \n",
        "    [`apply()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.core.groupby.GroupBy.apply.html) method, passing \n",
        "    `regress_okun` you wrote as the argument.\n",
        "\n",
        "\n",
        "    *Hint:* Use NumPy's \n",
        "    [`lstsq()`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html)\n",
        "    to perform the regression. To regress the dependent\n",
        "    variable `y` on regressors `X`, you need to call `lstsq(X, y)`.\n",
        "    To include the intercept, you manually have to create `X` such that the \n",
        "    first column contains only ones.\n",
        "\n",
        "6.  **[Advanced]** \n",
        "    Plot your results: for each decade, create a scatter plot of the raw \n",
        "    data and overlay it with the regression line you estimated."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "FIE463",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}