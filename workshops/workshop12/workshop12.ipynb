{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exercise 1: Predicting house prices with linear models\n",
        "\n",
        "In this exercise, you will work with the Ames housing data set which we already encountered in the lectures. Your task is to evaluate the following three linear models in terms of their performance when predicting house prices:\n",
        "\n",
        "1.  Linear regression without any regularization\n",
        "2.  Ridge regression\n",
        "3.  Lasso"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data description\n",
        "\n",
        "The data is stored in the file [`data/ames_houses.csv`](../../data/ames_houses.csv) and can be loaded as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Use this path to use the CSV file from the data/ directory\n",
        "file = '../../data/ames_houses.csv'\n",
        "\n",
        "df = pd.read_csv(file, sep=',')\n",
        "\n",
        "# Variables used in the analysis\n",
        "variables = [\n",
        "    'LotArea',\n",
        "    'LivingArea',\n",
        "    'Bathrooms',\n",
        "    'Bedrooms',\n",
        "    'SalePrice',\n",
        "    'OverallQuality',\n",
        "    'BuildingType',\n",
        "    'YearBuilt',\n",
        "    'CentralAir',\n",
        "]\n",
        "\n",
        "# Drop rows with any missing observation\n",
        "df = df.dropna(subset=variables)\n",
        "\n",
        "# Drop observations with large living or lot area\n",
        "df = df.query('LivingArea <= 350 & LotArea <= 5000')\n",
        "\n",
        "print(f'Number of observations: {df.shape[0]:,d}')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The included variables are a simplified subset of the original data (see [here](https://jse.amstat.org/v19n3/decock/DataDocumentation.txt) \n",
        "for a detailed description of the original variables):\n",
        "\n",
        "1. `LotArea`: Lot size in square meters\n",
        "2. `Neighborhood`: Physical locations within Ames city limits\n",
        "3. `OverallQuality`: Rates the overall material and finish of the house\n",
        "    (1 = very poor, 10 = excellent)\n",
        "4. `OverallCondition`: Rates the overall condition of the house\n",
        "    (1 = very poor, 10 = excellent)\n",
        "5. `YearBuilt`: Original construction date\n",
        "6. `YearRemodeled`: Remodel date (same as construction date if no remodeling or additions)\n",
        "7. `BuildingType`: Type of dwelling\n",
        "8. `CentralAir`: Central air conditioning (string, Y/N)\n",
        "9. `LivingArea`: Above grade (ground) living area in square meters\n",
        "10. `Bathrooms`: Full bathrooms above grade\n",
        "11. `Bedrooms`: Bedrooms above grade (does not include basement bedrooms)\n",
        "12. `Fireplaces`: Number of fireplaces\n",
        "13. `SalePrice`: Sale price in thousands of USD\n",
        "14. `YearSold`: Year sold\n",
        "15. `MonthSold`: Month sold\n",
        "16. `HasGarage`: Flag whether property has a garage"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***\n",
        "## Part 1 \u2014 Data preprocessing\n",
        "\n",
        "Apply the following steps to preprocess the data before estimation:\n",
        "\n",
        "1. Recode the string values in column `CentralAir` into numbers such that \n",
        "    `'N'` is mapped to 0 and `'Y'` is mapped to 1.\n",
        "2. Recode the string values in column `BuildingType` and create the new variable\n",
        "    `IsSingleFamily` which takes on the value 1 whenever a house is a \n",
        "    single-family home and 0 otherwise.\n",
        "3. Convert the variables `SalePrice`, `LivingArea` and `LotArea` to (natural) logs.\n",
        "    Name the transformed columns `logSalePrice`, `logLivingArea` and `logLotArea`.\n",
        "4.  Plot the histograms of `SalePrice`, `LivingArea`, and `LotArea`. In a new figure,\n",
        "    plot the histograms of `logSalePrice`, `logLivingArea` and `logLotArea`.\n",
        "    Which set of variables is better suited for model fitting?"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***\n",
        "## Part 2 \u2014 Model features\n",
        "\n",
        "### Model specification\n",
        "\n",
        "You are now asked to estimate the following model of house prices\n",
        "as a function of house characteristics:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\log(SalePrice_i) \n",
        "    &= \\alpha + f\\Bigl(\\log(LivingArea_i), ~\n",
        "        \\log(LotArea_i), OverallQuality_i,~ \\\\\n",
        "    & \\qquad \\qquad \\qquad  \n",
        "    Bathrooms_i,~ Bedrooms_i\\Bigr) \\\\\n",
        "    &+ \\gamma_0 YearBuilt_i + \n",
        "    \\gamma_1 CentralAir_i + \n",
        "    \\gamma_3 IsSingleFamily_i + \n",
        "    \\epsilon_i\n",
        "\\end{aligned}\n",
        "$$\n",
        "where $i$ indexes observations and $\\epsilon$ is an additive error term.\n",
        "The function $f(\\bullet)$ is a *polynomial of degree 3* in its\n",
        "arguments, i.e., it includes all terms and interactions of the given variables\n",
        "where the exponents sum to 3 or less:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "f(\\log(LivingArea_i), \\log(LotArea_i), \\dots)\n",
        "    &= \\beta_0 \\log(LivingArea_i) \n",
        "    + \\beta_1 \\log(LivingArea_i)^2 \\\\\n",
        "    &+ \\beta_2 \\log(LivingArea_i)^3 \n",
        "    + \\beta_3 \\log(LotArea_i) \\\\\n",
        "    &+ \\beta_4 \\log(LotArea_i)^2\n",
        "    + \\beta_5 \\log(LotArea_i)^3 \\\\\n",
        "    &+ \\beta_6 \\log(LivingArea_i)\\log(LotArea_i) \\\\\n",
        "    &+ \\beta_7 \\log(LivingArea_i)^2 \\log(LotArea_i) \\\\\n",
        "    &+ \\beta_8 \\log(LivingArea_i) \\log(LotArea_i)^2 \\\\\n",
        "    &+ \\cdots \n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Creating model features and outcomes\n",
        "\n",
        "\n",
        "1.  Complete the template code below to create a feature matrix `X` which contains all polynomial interactions as well as the remaining non-interacted variables.\n",
        "\n",
        "    *Hints:* \n",
        "\n",
        "    - Use the \n",
        "    [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)\n",
        "    transformation to create the polynomial terms and interactions from the columns\n",
        "        `logLivingArea`, `logLotArea`, `OverallQuality`, `Bathrooms` and  `Bedrooms`.\n",
        "    - Make sure that the generated polynomial does *not* contain a \n",
        "    constant (\"bias\"). You should include the intercept when estimating a model instead.\n",
        "    - You can use [`np.hstack()`](https://numpy.org/doc/stable/reference/generated/numpy.hstack.html) to concatenate two matrices (the polynomials and the remaining covariates) along the column dimension.\n",
        "    - The complete feature matrix `X` should contain a total of 58 columns (55 polynomial interactions and 3 non-polynomial features).\n",
        "\n",
        "2.  Split the data into a training and a test subset such that the training\n",
        "sample contains 70% of observations.\n",
        "\n",
        "    *Hint:* \n",
        "\n",
        "    - Use the function [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to split the sample.\n",
        "        Pass the argument `random_state=1234` to get reproducible results.\n",
        "    - Make sure to define the training and test samples only *once* so that they are identical for all estimators used below. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Random state (for train/test split and cross-validation)\n",
        "RANDOM_STATE = 1234      \n",
        "\n",
        "# Name of target variable\n",
        "target = 'logSalePrice'\n",
        "\n",
        "# Features included as polynomials\n",
        "features_poly = [\n",
        "    'logLivingArea',\n",
        "    'logLotArea',\n",
        "    'OverallQuality',\n",
        "    'Bathrooms',\n",
        "    'Bedrooms',\n",
        "]\n",
        "\n",
        "# Other features not included in polynomials\n",
        "features_other = ['YearBuilt', 'CentralAir', 'IsSingleFamily']\n",
        "features = features_poly + features_other\n",
        "\n",
        "# Keep only columns that are used to estimate model\n",
        "columns = [target] + features\n",
        "df = df[columns]\n",
        "\n",
        "# Response variable\n",
        "y = df[target]\n",
        "\n",
        "# TODO: Create polynomial features\n",
        "\n",
        "# TODO: Merge polynomial features and non-polynomial features into single matrix X\n",
        "\n",
        "# TODO: Split data into training and test sets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***\n",
        "## Part 3 \u2014 Linear regression\n",
        "\n",
        "Perform the following tasks:\n",
        "\n",
        "1. Estimate the above specification using the linear regression model \n",
        "    [`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) on the training sub-set. \n",
        "\n",
        "    -   Do you need to standardize features before estimating a linear regression model? \n",
        "    -   Does the linear regression model have any hyperparameters?\n",
        "\n",
        "2. Compute and report the root mean squared error (RMSE) and the $R^2$ on the test sample.\n",
        "\n",
        "*Hints:*\n",
        "\n",
        "- The root mean squared error can be computed with [`root_mean_squared_error()`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.root_mean_squared_error.html).\n",
        "\n",
        "- The $R^2$ can be computed with \n",
        "    [r2_score()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.r2_score.html)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***\n",
        "## Part 4 \u2014 Ridge regression\n",
        "\n",
        "Next, you want to estimate a Ridge regression which has the regularization\n",
        "strength $\\alpha$ as a hyperparameter.\n",
        "\n",
        "1. Use the template code below to run [`RidgeCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html)\n",
        "    to determine the best $\\alpha$ on the training sub-sample.\n",
        "    You can use the MSE metric (the default) to find the optimal $\\alpha$.\n",
        "    Report the optimal $\\alpha$ and the corresponding MSE.\n",
        "\n",
        "    - Does Ridge regression require feature standardization? If so, don't forget to apply it before fitting the model.\n",
        "2. Use the function `plot_validation_curve()` defined below to plot the MSE (averaged over folds on the training sub-sample) against the regularization strength $\\alpha$.\n",
        "3. Compute and report the RMSE and the $R^2$ on the test sample.\n",
        "\n",
        "*Hints:* \n",
        "-   Create `RidgeCV` with `store_cv_results=True` to store the MSEs on all folds. \n",
        "-   The MSEs for all folds and alphas are stored in the attribute `cv_results_` after fitting.\n",
        "-   The (negative!) best MSE is stored in the attribute `best_score_` after fitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_validation_curve(alphas, mse_mean, title=None):\n",
        "    \"\"\"\n",
        "    Plot validation curve for Ridge or Lasso.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alphas : array-like\n",
        "        Regularization strengths.\n",
        "    mse_mean : array-like\n",
        "        Cross-validated MSE (averaged over folds).\n",
        "    title : str, optional\n",
        "        Title of the plot.\n",
        "    \"\"\"\n",
        "\n",
        "    # Index of MSE-minimizing alpha\n",
        "    imin = np.argmin(mse_mean)\n",
        "\n",
        "    # Plot MSE against alphas, highlight minimum MSE\n",
        "    plt.plot(alphas, mse_mean)\n",
        "    plt.xlabel(r'Regularisation strength $\\alpha$ (log scale)')\n",
        "    plt.ylabel('Cross-validated MSE')\n",
        "    plt.scatter(alphas[imin], mse_mean[imin], s=15, c='black', zorder=100)\n",
        "    plt.axvline(alphas[imin], ls=':', lw=0.75, c='black')\n",
        "    plt.title(title)\n",
        "    plt.xscale('log')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Manually transform features\n",
        "\n",
        "# TODO: Create alpha grid uniformly spaced in logs on [1e-6, 100]\n",
        "# alphas = \n",
        "\n",
        "# TODO: Create RidgeCV and fit model\n",
        "# ridge_cv = \n",
        "\n",
        "# TODO: Report the best alpha and the corresponding MSE score\n",
        "\n",
        "# TODO: Compute MSEs averaged across folds (stored in cv_results_)\n",
        "# mse_mean = \n",
        "\n",
        "# TODO: Plot validation curve\n",
        "# plot_validation_curve(alphas, mse_mean)\n",
        "\n",
        "# TODO: compute and report RMSE and R2 on the test sample"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***\n",
        "## Part 5 \u2014 Lasso\n",
        "\n",
        "Next, you want to estimate a Lasso model which also has a regularization strength hyperparameter\n",
        "$\\alpha$:\n",
        "\n",
        "1. Use the template below to run [`LassoCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html)\n",
        "    to determine the best $\\alpha$ on the training sub-sample\n",
        "    using cross-validation with 5 folds.\n",
        "    You can use the MSE metric (the default) to find the optimal $\\alpha$.\n",
        "    Report the optimal $\\alpha$ and the corresponding MSE.\n",
        "\n",
        "    - Does Lasso require feature standardization? If so, don't forget to apply it before fitting the model.\n",
        "\n",
        "2. Use the function `plot_validation_curve()` to plot the MSE (averaged over folds on the training sub-sample) against the regularization strength $\\alpha$.\n",
        "3. Compute and report the RMSE and the $R^2$ on the test sample for the model using the optimal $\\alpha$.\n",
        "4. Report the number of non-zero coefficients for the model using the optimal $\\alpha$.\n",
        "\n",
        "*Hints:* \n",
        "\n",
        "- Getting Lasso to converge may require some experimentation. The following settings should help: \n",
        "\n",
        "    1.  Increase the max. number of iterations to `max_iter=100_000`.\n",
        "    2.  Use `selection='random'` and set `random_state=1234` to get reproducible results.\n",
        "\n",
        "- Use `eps=1.0e-4` as an argument to `LassoCV` to specify the ratio of the smallest to the largest $\\alpha$.\n",
        "- After cross-validation is complete, the MSE for each value of $\\alpha$ and each fold are stored in the attribute `mse_path_` which is an array with shape `(N_ALPHA, N_FOLDS)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Run cross-validation using LassoCV. Use the transformed features from earlier.\n",
        "# lasso_cv =\n",
        "\n",
        "# TODO: Report the best alpha (stored in alpha_) \n",
        "\n",
        "# TODO: Compute MSEs averaged across folds (stored in mse_path_)\n",
        "# mse_mean =\n",
        "\n",
        "# TODO: Plot validation curve\n",
        "# plot_validation_curve(lasso_cv.alphas_, mse_mean)\n",
        "\n",
        "# TODO: compute and report RMSE and R2 on the test sample\n",
        "\n",
        "# TODO: Report number of non-zero coefficients (stored in coef_)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***\n",
        "## Part 6 \u2014 Compare estimation results\n",
        "\n",
        "Create a table which contains the MSE and $R^2$ computed on the test sample for all three models (using their optimal hyperparameters). Which model performs best?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***\n",
        "# Exercise 2: Classification of above-average houses\n",
        "\n",
        "We continue with the setup from the previous exercise, but now use classification to predict whether a house was sold for more than the average price in its neighborhood.\n",
        "\n",
        "Use the same initial data processing steps as before, which are repeated here for convenience:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Use this path to use the CSV file from the data/ directory\n",
        "file = '../../data/ames_houses.csv'\n",
        "\n",
        "df = pd.read_csv(file, sep=',')\n",
        "\n",
        "# Drop rows with any missing observation\n",
        "df = df.dropna()\n",
        "\n",
        "# Drop observations with large living or lot area\n",
        "df = df.query('LivingArea <= 350 & LotArea <= 5000')\n",
        "\n",
        "# Create log-transformed variables\n",
        "df['logLivingArea'] = np.log(df['LivingArea'])\n",
        "df['logLotArea'] = np.log(df['LotArea'])\n",
        "\n",
        "# Create indicator variable for single family homes\n",
        "df['IsSingleFamily'] = (df['BuildingType'] == 'Single-family').astype(int)\n",
        "\n",
        "# Create indicator variable for central air\n",
        "df['CentralAir'] = df['CentralAir'].map({'Y': 1, 'N': 0})\n",
        "\n",
        "print(f'Number of observations: {df.shape[0]:,d}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***\n",
        "## Part 1 \u2014 Data preprocessing\n",
        "\n",
        "Perform the following additional data processing steps:\n",
        "\n",
        "1.  Drop all neighborhoods with less than 40 observations.\n",
        "2.  Create a new variable `MoreExpensive` which is 1 whenever the sale price is above \n",
        "    the average sale price in the neighborhood.\n",
        "3.  Split the data set into two data frames, `df_train` and `df_test`, where\n",
        "    the test sample should contain 20% of the observations.\n",
        "    Stratify the train-test split using the indicator `MoreExpensive`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***\n",
        "## Part 2 \u2014 Logistic regression\n",
        "\n",
        "Using the template code below, create the feature matrix for the logistic regression as follows:\n",
        "\n",
        "1.  Create polynomials of degree 3 using the variables `LivingArea`, `LotArea`,\n",
        "    `OverallQuality`, `OverallCondition`, `Bathrooms`, `Bedrooms`, `Fireplaces`, and\n",
        "    `YearRemodeled`\n",
        "\n",
        "2.  Add the non-interacted features \n",
        "    `CentralAir`, and `IsSingleFamily` to the feature matrix.\n",
        "\n",
        "Then perform the following steps to fit and evaluate the model:\n",
        "\n",
        "1.  Fit the logistic regression with \n",
        "[`LogisticRegression()`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html),\n",
        "using the indicator `MoreExpensive` as the target variable.\n",
        "\n",
        "    -   Does the logistic regression require feature standardization? If so,\n",
        "    you need to transform the features using\n",
        "    [`StandardScaler()`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
        "\n",
        "    -   You can use the default parameters for `LogisticRegression`, but you might need to increase\n",
        "    the maximum number of iterations (e.g., `max_iter=10_000`).\n",
        "\n",
        "2.  After you have fitted the model, use the function\n",
        "    `tabulate_classifier_metrics()` defined below to tabulate the accuracy,\n",
        "    precision, recall, and the F1 store on the test sample.\n",
        "\n",
        "3.  After you have fitted the model, use the function\n",
        "    `plot_confusion_matrix()` defined below to plot the confusion matrix\n",
        "    on the test sample.\n",
        "\n",
        "    This function calls\n",
        "    [ConfusionMatrixDisplay.from_estimator()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.ConfusionMatrixDisplay.html#sklearn.metrics.ConfusionMatrixDisplay.from_estimator)\n",
        "    to create a confusion matrix graph.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Target variable name\n",
        "target = 'MoreExpensive'\n",
        "\n",
        "# Features included as polynomials (in logistic regression)\n",
        "features_poly = [\n",
        "    'LivingArea',\n",
        "    'LotArea',\n",
        "    'OverallQuality',\n",
        "    'OverallCondition',\n",
        "    'Bathrooms',\n",
        "    'Bedrooms',\n",
        "    'Fireplaces',\n",
        "    'YearRemodeled'\n",
        "]\n",
        "\n",
        "# Other features not included in polynomials\n",
        "features_other = ['CentralAir', 'IsSingleFamily']\n",
        "features = features_poly + features_other\n",
        "\n",
        "# Response variable\n",
        "y_train = df_train[target]\n",
        "y_test = df_test[target]\n",
        "\n",
        "# TODO: Create polynomial features for training sample\n",
        "\n",
        "# TODO: Create polynomial features for test sample\n",
        "\n",
        "# TODO: Merge polynomial features and non-polynomial features into X_train\n",
        "\n",
        "# TODO: Merge polynomial features and non-polynomial features into X_test\n",
        "\n",
        "# TODO: Standardize features\n",
        "\n",
        "# TODO: Fit logistic regression model\n",
        "\n",
        "# TODO: Tabulate metrics on test sample using tabulate_classification_metrics()\n",
        "\n",
        "# TODO: Plot confusion matrix using plot_confusion_matrix()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "\n",
        "def tabulate_classifier_metrics(estimator, X, y):\n",
        "    \"\"\"\n",
        "    Tabulate classification metrics (accuracy, precision, recall, F1).\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : object\n",
        "        Fitted classifier.\n",
        "    X : array-like\n",
        "        Feature matrix.\n",
        "    y : array-like\n",
        "        Target variable.\n",
        "    \"\"\"\n",
        "\n",
        "    # Predict outcome\n",
        "    y_pred = estimator.predict(X)\n",
        "\n",
        "    # Compute scores\n",
        "    acc = accuracy_score(y, y_pred)\n",
        "    pre = precision_score(y, y_pred)\n",
        "    rec = recall_score(y, y_pred)\n",
        "    f1 = f1_score(y, y_pred)\n",
        "\n",
        "    # Combine scores into a single Series\n",
        "    index = pd.Index(\n",
        "        ['Accuracy', 'Precision [TP/(TP+FP)]', 'Recall [TP/P]', 'F1'], name='Metric'\n",
        "    )\n",
        "    stats = pd.Series([acc, pre, rec, f1], index=index)\n",
        "\n",
        "    stats = stats.round(3)\n",
        "\n",
        "    return stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "def plot_confusion_matrix(estimator, X, y, title='Confusion matrix'):\n",
        "    \"\"\"\n",
        "    Plot confusion matrix for classification model.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    estimator : estimator\n",
        "        Fitted classification model.\n",
        "    X : array-like\n",
        "        Feature matrix.\n",
        "    y : array-like\n",
        "        Target variable.\n",
        "    title : str\n",
        "        Title of the plot.\n",
        "    \"\"\"\n",
        "\n",
        "    cm = ConfusionMatrixDisplay.from_estimator(\n",
        "        estimator=estimator,\n",
        "        X=X,\n",
        "        y=y,\n",
        "        values_format=',d',\n",
        "        cmap='Blues',\n",
        "        colorbar=False,\n",
        "        text_kw={'fontsize': 10, 'fontweight': 'bold'},\n",
        "    )\n",
        "    cm.ax_.set_title(title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***\n",
        "## Part 3 \u2014 Logistic regression CV\n",
        "\n",
        "Instead of using the default regularization strength `C=1`, perform cross-validation \n",
        "to find the optimal value of $C$:\n",
        "\n",
        "1. Run the cross-validation with\n",
        "[LogisticRegressionCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html).\n",
        "\n",
        "    Create a log-spaced grid of candidate values as follows:\n",
        "\n",
        "    ```python\n",
        "    C_grid = np.logspace(-2, 2, 500)\n",
        "    ```\n",
        "2.  Report the optimal value of $C$.\n",
        "3.  After you have fitted the model, use the function\n",
        "    `tabulate_classifier_metrics()` to tabulate the accuracy,\n",
        "    precision, recall, and the F1 store on the test sample.\n",
        "\n",
        "3.  After you have fitted the model, use the function\n",
        "    `plot_confusion_matrix()` defined below to plot the confusion matrix\n",
        "    on the test sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***\n",
        "## Part 4 \u2014 Random forest\n",
        "\n",
        "You now want to investigate how other classifiers perform on this task compared\n",
        "to logistic regression.\n",
        "\n",
        "1.  Fit the Random forest classifier implemented in \n",
        "    [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to the data. Use the default parameters for now.\n",
        "\n",
        "    -   Do you need to include polynomial interactions with Random forest?\n",
        "    -   Do you need to standardize the features with Random forest?\n",
        "\n",
        "2.  After you have fitted the model, use the function\n",
        "    `tabulate_classifier_metrics()` to tabulate the accuracy,\n",
        "    precision, recall, and the F1 store on the test sample.\n",
        "\n",
        "3.  After you have fitted the model, use the function\n",
        "    `plot_confusion_matrix()` defined below to plot the confusion matrix\n",
        "    on the test sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***\n",
        "## Part 5 \u2014 Random forest CV\n",
        "\n",
        "In the previous part, you used the default hyperparameters for the Random\n",
        "forest (e.g., the number of trees to grow and the maximum depth).\n",
        "\n",
        "1.  Perform cross-validation of these parameters with\n",
        "    [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), using the parameter grids defined in the template below.\n",
        "\n",
        "2.  After you have fitted the model, use the function\n",
        "    `tabulate_classifier_metrics()` to tabulate the accuracy,\n",
        "    precision, recall, and the F1 store on the test sample.\n",
        "\n",
        "3.  After you have fitted the model, use the function\n",
        "    `plot_confusion_matrix()` defined below to plot the confusion matrix\n",
        "    on the test sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': np.arange(100, 201, 10),\n",
        "    'max_depth': np.arange(3, 20),\n",
        "}\n",
        "\n",
        "# TODO: Call GridSearchCV to find optimal hyperparameters\n",
        "\n",
        "# TODO: Report optimal number of estimators stored in best_params_\n",
        "\n",
        "# TODO: Report optimal max depth stored in best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "***\n",
        "## Part 6 \u2014 Compare estimation results\n",
        "\n",
        "Combine the accuracy, precision, recall, and F1 metrics for all the models you estimated and report them in a single table. Which estimator does best on the classification task?"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "FIE463",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}